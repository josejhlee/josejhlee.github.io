---
layout: post
title: "[PR] ManiGAN: Text-Guided Image Manipulation"
---

# Paper review
>paper link: https://arxiv.org/abs/1912.06203
>code link: https://github.com/mrlibw/ManiGAN/blob/master/code/model.py and https://github.com/mrlibw/ManiGAN (third party)
>notes: 
>tags: CVPR, GAN,  text-to-image, synthesis, 

## What?

## Why?

## How? But.. How?
Before we get into the details, let us go over some components in the architecture.  

**Affine Combination module(ACM)**: Given representation of text (embedded via a prior RNN) and image (embedded via a prior Inception) it fuses them. the representation/hidden features $h$ of the text is ChannelxHeightxWidth, and image representation $v$ is 256x17x17. Essentially, $h' = ACM(h,v)$. Now, ACM is the application of two convolution layers on $v$ to produce $W(v)$ and $bias(v)$, which will correspond to the shape of $v$. Then, a Hadamard product is applied to $W(v)$. The parameters $W_v, W_{bias}$ are learnable. So, you could think these parameters as tranferring from image feature space to text feature space. Then, the Hadamard product is more intuitive: it is the reweighting of each feature. Here, $W_{bias}$ is said to help the model to "reconstruct text-irrelevant contents."
$$h' = h\odot W(v)+b(v)$$
<p align="center">
  <img src="/assets/ManiGAN_ACM.png" />
</p>
Now, let's workout our way with the architecture.   
<p align="center">
  <img src="/assets/ManiGAN_architecture.png" />
</p> 

First, through respective encoders, we have the hiddens for both text and image. Firtly, at every stage, $h$ is upsampled through convolution layers. In ACM, where it meets image features, image features $v$ is matched to the respective shape of $h$ through convolutions. The output of AMC are fed into the generator of the corresponding stage to produce an edited image. It is also upsampled for the next stage. How each image generation differs from its previous one? The more you advance with the stages, the more "new visual attributes matches the given text description", and there is more reconstruction of text-irrelevant content. 

Next, we shift focus to the second module of the architecture.

**Detail Correction Module (DCM)**: It enhances detail and completes missing contents. It takes three inputs: the last hidden features $h_{last}$ from the last ACM, the word features encoded by some RNN, where each word is a feature vector, and visual features $v'$, which is 128x128x128, extracted from the input image. $v'$ is the relu2_2 layer representations from a pretrained VGG-16. There is another portion in DCM, which is the use of the attention mechanism.  

Word features and the last hidden features are fed as input for a spatial and channel-wise attention. Then, we will have $s, c$, both which are shape $\reals^{C'\times H'\times D'}$, which are concatenated and expressed as $a$.$a$ is referred to aid in refining visual attributes relevant to the given text, and a more accurate modification of the contents corresponding to the given description.

Now, in a bigger picture, $a$, which refines visual attributes that are relevant to the given text is served as the weights for the visual and shallow representations tilda $v'$ through ACM. (#notdone#)

<p align="center">
  <img src="/assets/ManiGAN_DCM.png" />
</p>

## What I did not understand...
Regional selection
shape of $h$. what does the feature map mean here?
How does bias take care of the text-irrelevant-content-reconstruction?
What is the word features referred in DCM?
In DCM, why does it use the Relu2_2 layer of VGG? It is said to be a shallow layer. Is it because that layer contains more global statistics?
In DCM. Lets say v' is v tilda(upsampled) for convenience's sake. There is ACM(a,v') and ACM(Res(ACM(a,v')), v'). What is the need of a residual training if ACM has a network itself? Is it because ACM has not approximated completely to the output we seek?
## Food for the brain
1. Why does ACM work better than a mere concatenation? That is hard to work well, given that the model does not know what we want from it when we do so. But when we introduce a neural network, we are explicitly saying what is the desired result.
2. 
