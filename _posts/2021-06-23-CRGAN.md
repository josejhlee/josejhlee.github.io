---
layout: post
title: "[PR] CONSISTENCY REGULARIZATION FOR GENERATIVE ADVERSARIAL NETWORKS"
---

# Paper review
>paper link:https://arxiv.org/abs/1910.12027
>code link: https://github.com/bluer555/CR-GAN (third party)
>notes: Should definately see the code due to its simplicity.
>Tags: Regularization, normalization, SOTA, 

## What?
Regularization method that augments data flowed to the "discriminator and penalize the sensitivity of the discriminator to these augmentations." That is, we want to train a discriminator that is dull to real data X with augmentation. It achieved lower FID score with various models and guarantee less compute demand. It works with different Losses and models and it is robust to hyperparameter tuning.

## Why?
While combining regularization and normalization tended to create conflict between the two "tools", the combination of the consistency reg. with, lets say, the spectral normalization actually gives enhanced performance. 

## How?
Consistency regularization is imported from the semi-supervised learning literature. The intuition is that augmentation should not affect the semantic meaning, because our domain of interest is not the pixel location but a more global--considering the overall image-- domain.
## But... how?
Take a look on the algorithm below. The highlighted section is the insertion of the regularization. So, We want to decrease the MSE between the D(x) and D(T(x)), where T(x) is a augmentation function. So, while the loss for the discriminator is
$$L^{cr}_D = L_D + \lambda L_{cr}$$ $$L^{cr}_G = L_G$$
The loss for the generator remains untouched.  
<img align="center" width="400" height="300" src="/assets/CR_algo.png">

## What I did not understand...
1. Why is ResNet used on the score table?
<img align="center" width="400" height="100" src="/assets/resnet_on_the_table.png">
## Food for the brain
1. Different augmentations preserve different levels of semantics. For example, Random shift & flip used in CR maintains FID low as 16.04, while Gaussian and cutout has higher FID (21.91 and 17,10 respectively). And a composition of cutout, rand shift & shift is worse then each individual's FID. 
