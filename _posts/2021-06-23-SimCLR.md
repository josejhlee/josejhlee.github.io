---
layout: post
title: "[PR] A Simple Framework for Contrastive Learning of Visual RepresentationsPaper title"
---

# Paper review
>paper link: https://arxiv.org/abs/2002.05709
>code link: https://github.com/sthalles/SimCLR
>notes: Worthy of looking at the code.
>tags: Contrastive learning, self-supervised, once-SOTA, 

## What?
They introduce SimCLR along with three contributions:1) highlights the importance of data augmentation in contrastive learning, 2) "introducing a nonlinear transformation between the representation and the constrastive loss (later referred as a projection), which increases the representation quality.
So, what is SimCLR? SimCLR learns "representations by maximizing agreement between differently augmented views of the same data example via a constractive loss in the latent space." If it is contrastive learning, there should follow along a contrastive loss function. The following it is.
![contrastive loss](contrastive_loss)

<p align="center">
  <img src="C:\Users\dell\Pictures\contrastive_loss.png" />
</p>


## Why?
In the real world, much of our data is not labelled. Self-supervision is useful when all we have is the unlabelled data. 
## How?
It is best explained with the diagram and the pseudocode. First the diagram.
![SimCLR_diagram](path)
$x$ undergoes two different augmentations $T()$ and $T'()$, hence creating two different image. These two become the input for a base encoder $f()$, which is responsible of extracting representation vectors $h$ from the images. In the paper, they adopt a ResNet. Notice that we ultimately want the avg. pooling layer in the last layer. This representation $h$ undergoes a projection head $g()$ that maps "representations to the space where contrastive loss is applied." Function $g()$ is a simple MLP with one hidden layer. The output is $z$. Then, the loss function has the goal of maximizing the agreement -in this case, the cosine similarity- between the two $z$'s.
## But.. how?
To be more specific, here is the pseudocode.
![SimCLR_pseudo](path)
One note. The loss function is calculated by computing the similarity between a sample $x_i$ and all $x_j$, where $j$ is not $i$. Also, there seems like the calculation for the loss is asymmetrical. Therefore, they calculate for swapped input $i$ and $j$ and take the mean of the two.
## What I did not understand...

## Food for the brain
1. Because it is reliant unto the comparison of other pairs (hence, contrastive), SimCLR's performance is very batch-size dependent.
2. The best augmentation is a combination of coloter distortion and croppipng. No single-transformation produces comparable result, which is interesting. Why that particular combination renders good performance?
3. I remember LeCunn professing in one of his blog post that contrastive method is not the "right" direction of self-supervised learning.
4. "Unsupervised learning benefits more from bigger models than its supervised counterpart." 
5. The projecting function $g()$ contains a non-linear function. This is about 3% better than a linear projection.